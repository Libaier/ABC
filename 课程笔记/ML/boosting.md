## Boosting

------

## 基本概念

Boosting算法是一种把若干个分类器整合为一个分类器的方法

 * 将多个弱分类器组合成为强分类器
 * 三个臭皮匠顶一个诸葛亮

前继方法

 * Bootstrapping
 * Bagging
 
------
## Adaboost算法

### 算法概述

输入: 训练样例

$$ T=\{(x\_{1},y\_{1}), (x\_{2},y\_{2}),...,(x\_{n},y\_{n})\} $$

输出: 由M个弱分类器构成的最终分类器G(x)

步骤:

初始化权值分布

$$ D\_{1}=(w\_{11},...,w\_{1i},...,w\_{1n}),w\_{1i}=\frac{1}{n} $$ 

对于m=1,2,…,M:

使用带权值的实例集合Dm训练模型，得到弱分类器:

$$G_{m}(x):x->y$$

计算Gm(x)在训练集上的误差率

$$e\_{m}=P(G\_{m}(x\_{i} \neq y\_{i}))=\sum\_{i=1}^{n}w\_{mi}I(G\_{m}(x\_{i}) \neq y\_{i})$$
计算Gm(x)的系数

$$a\_{m}=\frac{1}{2}ln\frac{1-e\_{m}}{e\_{m}}$$
这个地方用模型的整体误差来衡量弱分类器在最终分类器中的权重。

更新训练样例的权值分布，为下一轮迭代做准备

$$D\_{m+1}=(w\_{m+1,1},...,w\_{m+2,i},...,w\_{m+1,n})$$
$$w\_{m+1,i}=\frac{w\_{mi}}{Z\_{m}}exp(-a\_{m}y\_{i}G\_{m}(x\_{i}))$$
Zm是规范化因子:
$$Z\_{m}=\sum\_{i=1}^{n}w\_{mi}exp(-a\_{m}y\_{i}G\_{m}(x\_{i}))$$
$$$exp(-a\_{m}y\_{i}G\_{m}(x\_{i}))$$$这个部分，当分类正确时，整体<1；错误时，整体>1。意义是，当样例分类错误，我们加大它的权重，以便在后面的迭代中更受重视。相应的，降低分类正确的样例的权重。
进行了M轮迭代之后，产出了M个弱分类器，将他们组合起来:
$$f(x)=\sum\_{i=1}^{m}a\_{m}G\_{m}(x)$$

### 算法特点
* adaboost是一种有很高精度的分类器
* 可以使用各种方法构建子分类器，adaboost算法提供的是框架
* 当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单
* 简单，不用做特征筛选
* 不用担心overfitting！
　　
------
# 参考资料

 * [百度技术博客](http://baidutech.blog.51cto.com/4114344/743809)
 	
 * [技术博客](http://blog.crackcell.com/posts/2013/04/30/machine_learning_note_3_boosting.html)


