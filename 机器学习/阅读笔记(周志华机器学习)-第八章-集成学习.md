##8.1 个体与集成

也称多分类器系统，基于委员会的学习。

同质:基学习器

异质:组件学习器

弱学习器“好而不同”，容易获得更好的集成学习效果。

* 个体学习器间存在强依赖关系，必须**串行**生成的序列化方法->boosting
* 个体学习器间不存在强依赖关系，可同时生成的**并行**化方法->bagging、随机森林

##8.2 Boosting

* 先训练一个基学习器
* 根据基学习器的表现调节样本分布（侧重其分错的样本/可使用为样本赋予权值、重采样的方法实现）训练新的分类器
* 如此重复训练直到基学习器的数目达到指定的N
* 最终将这N个基学习器进行加权结合

最为著名的算法为[Adaboost](http://https://github.com/Libaier/ABC/blob/master/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/ML/boosting.pdf)。（不经修改只能适用于二分类）

其主要为了降低偏差。

##8.3 Bagging与随机森林

###Bagging
* 先使用bootstrap sampling对训练集采样
* 分别训练单独的基学习器
* 使用简单投票法确定结果

复杂度低，可不经修改直接用于多分类、回归，采样剩余样本可用于做验证集

其主要为了降低方差

###随机森林

采用bagging思想实现，且在每次进行进行最优属性划分选择时，随机选择K个子属性。这样同时引入了样本扰动和属性扰动。

简单，容易实现，训练与普通bagging相比效率高。

##8.4 结合策略

三个方面带来好处？

* 统计的方面
* 计算的方面
* 表示的方面

###平均法

* 简单平均
* 加权平均

###投票法

* 绝对多数投票(过半)
* 相对多数投票
* 加权投票

###学习法

使用另外一个学习器用来结合

Stacking，使用多响应线性回归MLR具有最佳表现。

贝叶斯模型平均(BMA)是另一种模型融合方法。

##8.5 多样性

###误差-分歧分解