##8.1 个体与集成

也称多分类器系统，基于委员会的学习。

同质:基学习器

异质:组件学习器

弱学习器“好而不同”，容易获得更好的集成学习效果。

* 个体学习器间存在强依赖关系，必须**串行**生成的序列化方法->boosting
* 个体学习器间不存在强依赖关系，可同时生成的**并行**化方法->bagging、随机森林

##8.2 Boosting

* 先训练一个基学习器
* 根据基学习器的表现调节样本分布（侧重其分错的样本/可使用为样本赋予权值、重采样的方法实现）训练新的分类器
* 如此重复训练直到基学习器的数目达到指定的N
* 最终将这N个基学习器进行加权结合

最为著名的算法为[Adaboost](http://https://github.com/Libaier/ABC/blob/master/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/ML/boosting.pdf)。（不经修改只能适用于二分类）

其主要为了降低偏差。

##8.3 Bagging与随机森林

###Bagging
* 先使用bootstrap sampling对训练集采样
* 分别训练单独的基学习器
* 使用简单投票法确定结果

复杂度低，可不经修改直接用于多分类、回归，采样剩余样本可用于做验证集

其主要为了降低方差

###随机森林

采用bagging思想实现，且在每次进行进行最优属性划分选择时，随机选择K个子属性。这样同时引入了样本扰动和属性扰动。

简单，容易实现，训练与普通bagging相比效率高。

##8.4 结合策略

三个方面带来好处？

* 统计的方面
* 计算的方面】
* 表示的方面

###平均法

* 简单平均
* 加权平均

###投票法

* 绝对多数投票(过半)
* 相对多数投票
* 加权投票

###学习法

使用另外一个学习器用来结合

Stacking，使用多响应线性回归MLR具有最佳表现。

贝叶斯模型平均(BMA)是另一种模型融合方法。

##8.5 多样性

###误差-分歧分解

个体学习器准确性越高，多样性越大，则集成越好。（只对回归问题有效）

###多样性度量
* 不合度量
* 相关系数
* Q-统计量
* k-统计量


*k误差图？？为什么感觉Adaboost和Bagging反了？*

###多样性增强

* 数据样本扰动(对不稳定基学习器Work:决策树，神经网络，对稳定基学习器不Work：线性学习器，支持向量机，朴素贝叶斯，k近邻学习器)
* 输入属性扰动*(随机子空间，特征？随机森林)*
* 输出表示扰动*（修改训练样本？？？）*
* 算法参数扰动

##8.6 阅读材料

[Gradient Boosting](http://http://www.cnblogs.com/LeftNotEasy/archive/2011/01/02/machine-learning-boosting-and-gradient-boosting.html)是一种Boosting的方法，它主要的思想是，每一次建立模型是在之前建立模型损失函数的梯度下降方向。Boosting主要是一种思想，表示“知错就改”。而Gradient Boosting是在这个思想下的一种函数（也可以说是模型）的优化的方法，首先将函数分解为可加的形式（其实所有的函数都是可加的，只是是否好放在这个框架中，以及最终的效果如何）。然后进行m次迭代，通过使得损失函数在梯度方向上减少，最终得到一个优秀的模型。

*[Cost function/Lost function??](http://http://blog.csdn.net/dark_scope/article/details/24863289)*
